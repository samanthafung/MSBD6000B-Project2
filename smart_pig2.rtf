{\rtf1\ansi\ansicpg1252\cocoartf1561\cocoasubrtf100
{\fonttbl\f0\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;}
{\*\expandedcolortbl;;\csgray\c0;\csgray\c100000;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs22 \cf2 \cb3 \CocoaLigature0 (tensorflow_p36) [ec2-user@ip-172-31-12-250 project2]$ sudo python3 smart_pig2.py \
Using TensorFlow backend.\
Found 3119 images belonging to 5 classes.\
Found 550 images belonging to 5 classes.\
_________________________________________________________________\
Layer (type)                 Output Shape              Param #   \
=================================================================\
conv2d_1 (Conv2D)            (None, 62, 62, 64)        1792      \
_________________________________________________________________\
activation_1 (Activation)    (None, 62, 62, 64)        0         \
_________________________________________________________________\
max_pooling2d_1 (MaxPooling2 (None, 31, 31, 64)        0         \
_________________________________________________________________\
conv2d_2 (Conv2D)            (None, 29, 29, 32)        18464     \
_________________________________________________________________\
activation_2 (Activation)    (None, 29, 29, 32)        0         \
_________________________________________________________________\
max_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \
_________________________________________________________________\
dropout_1 (Dropout)          (None, 14, 14, 32)        0         \
_________________________________________________________________\
conv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     \
_________________________________________________________________\
activation_3 (Activation)    (None, 12, 12, 64)        0         \
_________________________________________________________________\
max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \
_________________________________________________________________\
flatten_1 (Flatten)          (None, 2304)              0         \
_________________________________________________________________\
dense_1 (Dense)              (None, 128)               295040    \
_________________________________________________________________\
activation_4 (Activation)    (None, 128)               0         \
_________________________________________________________________\
dropout_2 (Dropout)          (None, 128)               0         \
_________________________________________________________________\
dense_2 (Dense)              (None, 5)                 645       \
_________________________________________________________________\
activation_5 (Activation)    (None, 5)                 0         \
=================================================================\
Total params: 334,437\
Trainable params: 334,437\
Non-trainable params: 0\
_________________________________________________________________\
None\
Epoch 1/80\
2017-11-22 12:35:38.016439: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\
98/97 [==============================] - 14s 140ms/step - loss: 1.4534 - acc: 0.3472 - val_loss: 1.3080 - val_acc: 0.4055\
Epoch 2/80\
98/97 [==============================] - 13s 131ms/step - loss: 1.2359 - acc: 0.4605 - val_loss: 1.1618 - val_acc: 0.5073\
Epoch 3/80\
98/97 [==============================] - 13s 130ms/step - loss: 1.1477 - acc: 0.5256 - val_loss: 1.1177 - val_acc: 0.5473\
Epoch 4/80\
98/97 [==============================] - 13s 131ms/step - loss: 1.0937 - acc: 0.5488 - val_loss: 1.0521 - val_acc: 0.5745\
Epoch 5/80\
98/97 [==============================] - 13s 131ms/step - loss: 1.0364 - acc: 0.5881 - val_loss: 0.9901 - val_acc: 0.6236\
Epoch 6/80\
98/97 [==============================] - 13s 128ms/step - loss: 1.0008 - acc: 0.6016 - val_loss: 0.9719 - val_acc: 0.6164\
Epoch 7/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.9667 - acc: 0.6122 - val_loss: 0.8987 - val_acc: 0.6564\
Epoch 8/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.9317 - acc: 0.6420 - val_loss: 0.9189 - val_acc: 0.6418\
Epoch 9/80\
98/97 [==============================] - 13s 131ms/step - loss: 0.8956 - acc: 0.6524 - val_loss: 0.9034 - val_acc: 0.6145\
Epoch 10/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.8830 - acc: 0.6571 - val_loss: 0.8656 - val_acc: 0.6655\
Epoch 11/80\
98/97 [==============================] - 13s 131ms/step - loss: 0.8517 - acc: 0.6720 - val_loss: 0.8076 - val_acc: 0.6727\
Epoch 12/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.8552 - acc: 0.6648 - val_loss: 0.8273 - val_acc: 0.6600\
Epoch 13/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.8284 - acc: 0.6714 - val_loss: 0.7713 - val_acc: 0.7073\
Epoch 14/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.8226 - acc: 0.6802 - val_loss: 0.7745 - val_acc: 0.7109\
Epoch 15/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.7992 - acc: 0.6945 - val_loss: 0.7555 - val_acc: 0.7200\
Epoch 16/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.7984 - acc: 0.6964 - val_loss: 0.7265 - val_acc: 0.7200\
Epoch 17/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.7535 - acc: 0.7117 - val_loss: 0.6913 - val_acc: 0.7382\
Epoch 18/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.7716 - acc: 0.7046 - val_loss: 0.6930 - val_acc: 0.7236\
Epoch 19/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.7427 - acc: 0.7130 - val_loss: 0.6971 - val_acc: 0.7273\
Epoch 20/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.7412 - acc: 0.7205 - val_loss: 0.6929 - val_acc: 0.7309\
Epoch 21/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.7338 - acc: 0.7233 - val_loss: 0.6625 - val_acc: 0.7418\
Epoch 22/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.7068 - acc: 0.7277 - val_loss: 0.6570 - val_acc: 0.7418\
Epoch 23/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.7046 - acc: 0.7330 - val_loss: 0.6599 - val_acc: 0.7345\
Epoch 24/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.6901 - acc: 0.7321 - val_loss: 0.6580 - val_acc: 0.7345\
Epoch 25/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.6835 - acc: 0.7439 - val_loss: 0.6375 - val_acc: 0.7455\
Epoch 26/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.6771 - acc: 0.7465 - val_loss: 0.6119 - val_acc: 0.7455\
Epoch 27/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6689 - acc: 0.7473 - val_loss: 0.6053 - val_acc: 0.7655\
Epoch 28/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6527 - acc: 0.7437 - val_loss: 0.5839 - val_acc: 0.7636\
Epoch 29/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.6592 - acc: 0.7553 - val_loss: 0.5678 - val_acc: 0.7673\
Epoch 30/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6402 - acc: 0.7551 - val_loss: 0.6506 - val_acc: 0.7436\
Epoch 31/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6482 - acc: 0.7540 - val_loss: 0.5810 - val_acc: 0.7982\
Epoch 32/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.6285 - acc: 0.7680 - val_loss: 0.5423 - val_acc: 0.7836\
Epoch 33/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6177 - acc: 0.7595 - val_loss: 0.5747 - val_acc: 0.7782\
Epoch 34/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6186 - acc: 0.7608 - val_loss: 0.5359 - val_acc: 0.7891\
Epoch 35/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6220 - acc: 0.7604 - val_loss: 0.5313 - val_acc: 0.7982\
Epoch 36/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.6055 - acc: 0.7749 - val_loss: 0.5158 - val_acc: 0.7964\
Epoch 37/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5934 - acc: 0.7764 - val_loss: 0.5151 - val_acc: 0.7855\
Epoch 38/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.6114 - acc: 0.7626 - val_loss: 0.5148 - val_acc: 0.7891\
Epoch 39/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5773 - acc: 0.7800 - val_loss: 0.5094 - val_acc: 0.8091\
Epoch 40/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.5847 - acc: 0.7868 - val_loss: 0.5011 - val_acc: 0.8036\
Epoch 41/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.5749 - acc: 0.7782 - val_loss: 0.5271 - val_acc: 0.8000\
Epoch 42/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.5792 - acc: 0.7764 - val_loss: 0.6014 - val_acc: 0.7564\
Epoch 43/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5536 - acc: 0.7892 - val_loss: 0.4759 - val_acc: 0.8145\
Epoch 44/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5609 - acc: 0.7846 - val_loss: 0.4894 - val_acc: 0.8273\
Epoch 45/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5506 - acc: 0.7893 - val_loss: 0.4590 - val_acc: 0.8364\
Epoch 46/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5423 - acc: 0.7990 - val_loss: 0.5030 - val_acc: 0.8291\
Epoch 47/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5393 - acc: 0.7946 - val_loss: 0.4642 - val_acc: 0.8218\
Epoch 48/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5355 - acc: 0.8052 - val_loss: 0.4524 - val_acc: 0.8255\
Epoch 49/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5300 - acc: 0.8095 - val_loss: 0.4587 - val_acc: 0.8327\
Epoch 50/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.5307 - acc: 0.8042 - val_loss: 0.4510 - val_acc: 0.8400\
Epoch 51/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.5263 - acc: 0.8025 - val_loss: 0.4530 - val_acc: 0.8327\
Epoch 52/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.5276 - acc: 0.7944 - val_loss: 0.4802 - val_acc: 0.8073\
Epoch 53/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.5203 - acc: 0.8056 - val_loss: 0.4708 - val_acc: 0.8436\
Epoch 54/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.5017 - acc: 0.8015 - val_loss: 0.4032 - val_acc: 0.8509\
Epoch 55/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.5016 - acc: 0.8202 - val_loss: 0.4068 - val_acc: 0.8436\
Epoch 56/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4890 - acc: 0.8148 - val_loss: 0.4874 - val_acc: 0.8091\
Epoch 57/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.4992 - acc: 0.8084 - val_loss: 0.4200 - val_acc: 0.8455\
Epoch 58/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.4946 - acc: 0.8115 - val_loss: 0.4081 - val_acc: 0.8455\
Epoch 59/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4892 - acc: 0.8217 - val_loss: 0.4235 - val_acc: 0.8345\
Epoch 60/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.4820 - acc: 0.8177 - val_loss: 0.3766 - val_acc: 0.8745\
Epoch 61/80\
98/97 [==============================] - 13s 128ms/step - loss: 0.4819 - acc: 0.8146 - val_loss: 0.3880 - val_acc: 0.8545\
Epoch 62/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4825 - acc: 0.8243 - val_loss: 0.3747 - val_acc: 0.8618\
Epoch 63/80\
98/97 [==============================] - 12s 127ms/step - loss: 0.4650 - acc: 0.8215 - val_loss: 0.3654 - val_acc: 0.8800\
Epoch 64/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4512 - acc: 0.8333 - val_loss: 0.3713 - val_acc: 0.8473\
Epoch 65/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4536 - acc: 0.8255 - val_loss: 0.4095 - val_acc: 0.8436\
Epoch 66/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4499 - acc: 0.8397 - val_loss: 0.3909 - val_acc: 0.8582\
Epoch 67/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4505 - acc: 0.8361 - val_loss: 0.3504 - val_acc: 0.8618\
Epoch 68/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4530 - acc: 0.8298 - val_loss: 0.3625 - val_acc: 0.8764\
Epoch 69/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.4499 - acc: 0.8321 - val_loss: 0.3673 - val_acc: 0.8764\
Epoch 70/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4343 - acc: 0.8308 - val_loss: 0.3337 - val_acc: 0.8800\
Epoch 71/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4352 - acc: 0.8449 - val_loss: 0.3704 - val_acc: 0.8618\
Epoch 72/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.4403 - acc: 0.8347 - val_loss: 0.3506 - val_acc: 0.8636\
Epoch 73/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.4224 - acc: 0.8378 - val_loss: 0.3496 - val_acc: 0.9036\
Epoch 74/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4142 - acc: 0.8440 - val_loss: 0.3477 - val_acc: 0.8891\
Epoch 75/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4231 - acc: 0.8400 - val_loss: 0.3139 - val_acc: 0.8982\
Epoch 76/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4001 - acc: 0.8468 - val_loss: 0.3291 - val_acc: 0.8764\
Epoch 77/80\
98/97 [==============================] - 13s 129ms/step - loss: 0.4120 - acc: 0.8506 - val_loss: 0.3326 - val_acc: 0.8818\
Epoch 78/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.3983 - acc: 0.8512 - val_loss: 0.3189 - val_acc: 0.8891\
Epoch 79/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.4101 - acc: 0.8453 - val_loss: 0.3818 - val_acc: 0.8745\
Epoch 80/80\
98/97 [==============================] - 13s 130ms/step - loss: 0.3994 - acc: 0.8490 - val_loss: 0.2996 - val_acc: 0.8927\
\
acc: 89.27%\
}